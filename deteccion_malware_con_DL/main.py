from omegaconf import OmegaConf
import argparse
from models import get_model
from data import get_dataloaders
from optimizers import get_optimizer
from losses import get_loss
from schedulers import get_scheduler
from metrics import get_metrics
from callbacks import get_callbacks
from trainers.base_trainer import BaseTrainer
from loggers import setup_logger, get_output_logger
from utils.loggers import setup_logger
from utils.seed import seed_everything
from utils.get_experiment_id import get_experiment_id
from utils.load_checkpoint import load_checkpoint
from utils.wandb_login import wandb_login
from utils.filter_wrong_predictions import filter_wrong_predictions
import torch
import wandb
import csv
import os


def main(config_path):
    config = OmegaConf.load(config_path)
    seed_everything(config.training.seed)
    config.experiment_id = get_experiment_id(config)

    wandb_login()
    logger = setup_logger()
    logger.info("Configuración cargada:")
    logger.info(config)

    if config.training.device == "cuda":
        device = "cuda" if torch.cuda.is_available() else "cpu"
        config.training.device = device

    # El dataset de competición no tiene validación, asi que devuelve null,
    # Dado el contexto de la competición el test viene sin las etiquetas
    # Se debe entregar un csv, separado por comas, con las etiquetas de test, respetando el orden de test.
    train_loader, val_loader, test_loader = get_dataloaders(config)
    print(
        f"train_loader: {train_loader}, val_loader: {val_loader}, test_loader: {test_loader}"
    )

    model = get_model(config.model).to(config.training.device)
    criterion = get_loss(config.loss)
    optimizer = get_optimizer(config.optimizer, model.parameters())
    scheduler = get_scheduler(config.scheduler, optimizer)
    callbacks = get_callbacks(config.callbacks)
    metrics = get_metrics(config.metrics)

    trainer = BaseTrainer(
        model, criterion, optimizer, scheduler, config, logger, callbacks, metrics
    )
    trainer.train(train_loader, val_loader)

    # Solo cargar el checkpoint si existe
    checkpoint_path = f"checkpoints/{config.experiment_id}/best.pth"
    if os.path.exists(checkpoint_path):
        load_checkpoint(model, config)
    else:
        print(f"No se encontró el checkpoint {checkpoint_path}, se usará el modelo actual para predicción.")

    val_metrics, inputs, outputs, targets = trainer.run_epoch(
        val_loader, mode="Val", return_preds=True
    )
    print("Métricas de validación:")
    print(val_metrics)

    # Generar predicciones para test y guardarlas en un CSV solo si test_loader no es None
    if test_loader is not None:
        test_preds = []
        model.eval()
        with torch.no_grad():
            for batch in test_loader:
                if isinstance(batch, (list, tuple)):
                    inputs = batch[0]
                else:
                    inputs = batch
                inputs = inputs.to(config.training.device)
                outputs = model(inputs)
                preds = outputs.argmax(dim=1).cpu().numpy()
                test_preds.extend(preds)
        with open("EquipoReturn_predicciones.csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(test_preds)
        print("Archivo de predicciones generado: EquipoReturn_predicciones.csv")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/example.yaml")
    args = parser.parse_args()
    main(args.config)
